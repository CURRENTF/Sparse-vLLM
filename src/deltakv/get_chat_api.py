import os
import sys
import torch
from typing import Union, List
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
from deltakv.configs.model_config_cls import KVQwen2Config, KVLlamaConfig
from safetensors.torch import load_file


def load_compressor(compressor_path, device='cuda:0'):
    state_dict = load_file(os.path.join(compressor_path, 'model.safetensors'), device)
    return state_dict


def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float("Inf"), min_tokens_to_keep=1):
    """ Filter a distribution of logits using top-k and/or nucleus (top-p) filtering
        Args:
            logits: logits distribution shape (batch, vocabulary size)
            if top_k > 0: keep only top k tokens with the highest probability (top-k filtering).
            if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).
                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)
            Make sure we keep at least min_tokens_to_keep per batch example in the end
        From: https://gist.github.com/thomwolf/1a5a29f69620886c271b93575f97Self
    """
    if top_k > 0:
        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check
        # Remove all tokens with a probability less than the last token of the top-k
        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
        logits[indices_to_remove] = filter_value

    if top_p < 1.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)

        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)
        sorted_indices_to_remove = cumulative_probs > top_p
        if min_tokens_to_keep > 1:
            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep - 1 because we add the first one below)
            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0
        # Shift the indices to the right to keep also the first token above the threshold
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0

        # scatter sorted tensors to original indexing
        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
        logits[indices_to_remove] = filter_value
    return logits


@torch.inference_mode()
def manual_generate(model, tokenizer, prompt: Union[str, List[str]], past_key_values=None, return_kv_cache=False, **kwargs):
    """
    æ‰‹åŠ¨å®ç°çš„ç”Ÿæˆå‡½æ•°ï¼Œæ”¯æŒKV Cacheå¤ç”¨åŠé‡‡æ ·ã€‚
    """
    if isinstance(prompt, str):
        prompts = [prompt]
        is_single = True
    else:
        prompts = prompt
        is_single = False

    max_new_tokens = kwargs.get('max_new_tokens', 128)
    do_sample = kwargs.get('do_sample', False)
    temperature = kwargs.get('temperature', 1.0)
    top_k = kwargs.get('top_k', 50)
    top_p = kwargs.get('top_p', 1.0)
    add_special_tokens = True
    if tokenizer.bos_token is None or prompts[0].startswith(tokenizer.bos_token):
        add_special_tokens = False
    inputs = tokenizer(prompts, return_tensors='pt', padding=True, add_special_tokens=add_special_tokens).to(model.device)
    input_ids = inputs.input_ids

    batch_size = input_ids.shape[0]
    unfinished_sequences = input_ids.new_ones(batch_size, device=model.device)

    # ä¿å­˜æ¯ä¸€è½®ç”Ÿæˆçš„ token
    generated_tokens = []

    cur_input_ids = input_ids
    # æ”¯æŒåˆ†å— Prefill ä»¥é™ä½æ¿€æ´»æ˜¾å­˜å ç”¨
    chunk_prefill_size = int(os.environ.get('MANUAL_GEN_CHUNK_PREFILL_SIZE', 0))
    if chunk_prefill_size > 0 and input_ids.shape[1] > chunk_prefill_size:
        seq_len = input_ids.shape[1]
        for j in range(0, seq_len - 1, chunk_prefill_size):
            end_idx = min(j + chunk_prefill_size, seq_len - 1)
            chunk = input_ids[:, j:end_idx]
            outputs = model(input_ids=chunk, past_key_values=past_key_values, use_cache=True)
            past_key_values = outputs.past_key_values
        cur_input_ids = input_ids[:, -1:]

    eos_token_ids = kwargs.get('eos_token_id', [tokenizer.eos_token_id])
    if isinstance(eos_token_ids, int):
        eos_token_ids = [eos_token_ids]
    eos_token_ids_tensor = torch.tensor(eos_token_ids, device=model.device)

    for i in range(max_new_tokens):
        outputs = model(input_ids=cur_input_ids, past_key_values=past_key_values, use_cache=True)
        logits = outputs.logits[:, -1, :]
        past_key_values = outputs.past_key_values

        if os.environ.get('BAN_EOS') and tokenizer.eos_token_id is not None:
            logits[:, tokenizer.eos_token_id] = -float('inf')

        if do_sample:
            if temperature != 1.0:
                logits = logits / temperature
            logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)
            probs = torch.softmax(logits, dim=-1)
            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
        else:
            next_tokens = torch.argmax(logits, dim=-1)

        # è€ƒè™‘ EOS (æ”¯æŒå¤šä¸ª EOS ID)
        is_eos = torch.isin(next_tokens, eos_token_ids_tensor)
        next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)
        generated_tokens.append(next_tokens)

        # æ›´æ–°çŠ¶æ€
        unfinished_sequences = unfinished_sequences.mul((~is_eos).long())
        if unfinished_sequences.max() == 0:
            break

        cur_input_ids = next_tokens.unsqueeze(-1)

    skip_special_tokens = True
    if os.environ.get('NOT_SKIP_SPECIAL_TOKENS', False):
        skip_special_tokens = False

    # æ‹¼æ¥å¹¶è§£ç 
    if not generated_tokens:
        results = ["" for _ in range(batch_size)]
    else:
        all_generated_ids = torch.stack(generated_tokens, dim=1)
        results = []
        for i in range(batch_size):
            text = tokenizer.decode(all_generated_ids[i], skip_special_tokens=skip_special_tokens)
            results.append(text)

    if return_kv_cache:
        return (results[0], past_key_values) if is_single else (results, past_key_values)
    return results[0] if is_single else results


def hf_gen(model, tokenizer, prompt: Union[str, List[str]], return_kv_cache, past_key_values=None, **kwargs):
    assert past_key_values is None
    if isinstance(prompt, str):
        prompts = [prompt]
        is_single = True
    else:
        prompts = prompt
        is_single = False

    inputs = tokenizer(prompts, return_tensors='pt', padding=True, add_special_tokens=False).to(model.device)

    gen_config = {
        "max_new_tokens": kwargs.get('max_new_tokens', 128),
        "do_sample": kwargs.get('do_sample', False),
        "temperature": kwargs.get('temperature', 1.0),
        "top_k": kwargs.get('top_k', 50),
        "top_p": kwargs.get('top_p', 1.0),
        "eos_token_id": kwargs.get('eos_token_id', tokenizer.eos_token_id),
        "pad_token_id": tokenizer.pad_token_id,
        "use_cache": True,
    }

    # å¦‚æœä¸é‡‡æ ·ï¼Œç§»é™¤ç›¸å…³å‚æ•°ä»¥é¿å…è­¦å‘Š
    if not gen_config["do_sample"]:
        gen_config.pop("temperature", None)
        gen_config.pop("top_k", None)
        gen_config.pop("top_p", None)

    output_ids = model.generate(**inputs, **gen_config)

    input_len = inputs.input_ids.shape[1]
    generated_ids = output_ids[:, input_len:]

    results = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

    if return_kv_cache:
        return (results[0], None) if is_single else (results, None)
    return results[0] if is_single else results


def get_generate_api(model_path: str, infer_config: dict, compressor_path: str,
                     tokenizer_path: str = None, model_cls: str = 'deltakv',  use_cache: bool = True,
                     cuda_device: Union[int, str] = 0, backend: str = 'hf', return_kv_cache: bool = False,
                     return_model: bool = False):
    """
    Returns:
        function: ä¸€ä¸ªç”Ÿæˆå‡½æ•°ï¼Œè¾“å…¥promptå’Œç”Ÿæˆå‚æ•°ï¼Œè¿”å›ç”Ÿæˆå†…å®¹ã€‚
    """

    if backend == 'sparsevllm':
        from sparsevllm import LLM, SamplingParams
        # sparsevllm å†…éƒ¨ç®¡ç† tokenizer å’Œè®¾å¤‡
        # TODO è¿™è¾¹é€»è¾‘ç¨å¾®ä¸å¤ªç»Ÿä¸€ï¼Œä¸é€šè¿‡ compressor_path ä¼  compressor
        
        llm = LLM(
            model_path, 
            **infer_config,
        )

        def generate(prompt: Union[str, List[str]], past_key_values=None, **kwargs):
            if isinstance(prompt, str):
                prompts = [prompt]
                is_single = True
            else:
                prompts = prompt
                is_single = False

            # å°† HF é£æ ¼å‚æ•°æ˜ å°„åˆ° SamplingParams
            max_tokens = kwargs.get('max_new_tokens', kwargs.get('max_tokens', 128))
            temperature = kwargs.get('temperature', 1.0)
            
            # å¦‚æœæ˜¯ greedy (do_sample=False)ï¼ŒSamplingParams éœ€è¦ç‰¹æ®Šå¤„ç†æˆ–ç”±ç”¨æˆ·ä¿è¯æ¸©æ§åˆ¶
            if not kwargs.get('do_sample', True):
                temperature = 1e-5
            
            if temperature < 1e-5: temperature = 1e-5
            
            sampling_params = SamplingParams(temperature=temperature, max_tokens=max_tokens)
            outputs = llm.generate(prompts, sampling_params, use_tqdm=False)
            
            results = [out['text'] for out in outputs]
            if return_kv_cache:
                return (results[0], None) if is_single else (results, None)
            return results[0] if is_single else results
        
        if return_model:
            raise ValueError('sparse vllm ä¸æ”¯æŒ return_model=True')
        return generate

    assert use_cache, 'è¿˜è¦åšpaddingæ‰èƒ½ç”¨è®­ç»ƒä»£ç æ¨ç†'
    if model_cls == 'deltakv':
        base_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        if base_config.model_type == 'qwen2':
            if use_cache:
                from deltakv.modeling.qwen2.qwen2_with_compress_inference import Qwen2KVCompress as KVModel
            else:
                from deltakv.modeling.qwen2.qwen2_e2e import Qwen2KVCompress as KVModel
            config_cls = KVQwen2Config
        elif base_config.model_type == 'llama':
            from deltakv.modeling.llama.llama_with_compress_inference import LlamaKVCompress as KVModel
            config_cls = KVLlamaConfig
        else:
            raise ValueError(f"Unsupported model type: {base_config.model_type}")

        if compressor_path is not None:
            config = config_cls.from_pretrained(compressor_path)
        else:
            config = config_cls.from_pretrained(model_path)
        config.set_infer_args(**infer_config)
        model = KVModel.from_pretrained(
            model_path,
            config=config,
            torch_dtype=torch.bfloat16,
            device_map=cuda_device,
            attn_implementation="flash_attention_2",
        )
        if compressor_path is not None:
            load_device = f'cuda:{cuda_device}' if isinstance(cuda_device, int) else 'cpu'
            comp_state_dict = load_compressor(compressor_path, device=load_device)
            _, unexpected = model.load_state_dict(comp_state_dict, strict=False)
            assert len(unexpected) == 0, f'compressor åŠ è½½æœ‰é—®é¢˜: {unexpected}'
            del comp_state_dict

    elif model_cls == 'snapkv':
        base_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        if base_config.model_type == 'qwen2':
            from deltakv.modeling.qwen2.qwen2_snapkv import Qwen2SnapKVForCausalLM as KVModel
            config_cls = KVQwen2Config
        elif base_config.model_type == 'llama':
            from deltakv.modeling.llama.llama_snapkv import LlamaSnapKVForCausalLM as KVModel
            config_cls = KVLlamaConfig
        else:
            raise ValueError(f"Unsupported model type: {base_config.model_type}")

        print('ğŸ’¡ğŸ’¡ğŸ’¡ SnapKV')
        config = config_cls.from_pretrained(model_path)
        config.set_infer_args(**infer_config)
        print(f'[Config] {config}')
        model = KVModel.from_pretrained(
            model_path,
            config=config,
            torch_dtype=torch.bfloat16,
            device_map=cuda_device,
            attn_implementation="flash_attention_2",
        )

    elif model_cls == 'pyramidkv':
        base_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        if base_config.model_type == 'qwen2':
            from deltakv.modeling.qwen2.qwen2_pyramidkv import Qwen2PyramidKVForCausalLM as KVModel
            config_cls = KVQwen2Config
        elif base_config.model_type == 'llama':
            from deltakv.modeling.llama.llama_pyramidkv import LlamaPyramidKVForCausalLM as KVModel
            config_cls = KVLlamaConfig
        else:
            raise ValueError(f"Unsupported model type: {base_config.model_type}")

        print('ğŸ’¡ğŸ’¡ğŸ’¡ PyramidKV')
        config = config_cls.from_pretrained(model_path)
        config.set_infer_args(**infer_config)
        print(f'[Config] {config}')
        model = KVModel.from_pretrained(
            model_path,
            config=config,
            torch_dtype=torch.bfloat16,
            device_map=cuda_device,
            attn_implementation="flash_attention_2",
        )

    elif model_cls == 'auto':
        print('ğŸ’¡ğŸ’¡ğŸ’¡ Auto')
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=cuda_device,
            attn_implementation="flash_attention_2",
        )
        chunk_prefill_size = infer_config.get('chunk_prefill_size', None)
        if chunk_prefill_size is not None:
            from types import MethodType

            def chunked_forward(self, input_ids=None, past_key_values=None, **kwargs):
                if input_ids is not None and input_ids.shape[1] > chunk_prefill_size:
                    seq_len = input_ids.shape[1]
                    outputs = None
                    for i in range(0, seq_len, chunk_prefill_size):   # noqa
                        chunk = input_ids[:, i:i + chunk_prefill_size]   # noqa
                        outputs = self.original_forward(input_ids=chunk, past_key_values=past_key_values, **kwargs)
                        past_key_values = outputs.past_key_values
                    return outputs
                return self.original_forward(input_ids=input_ids, past_key_values=past_key_values, **kwargs)

            print('monkey patch raw full attn')
            model.original_forward = model.forward
            model.forward = MethodType(chunked_forward, model)

    elif model_cls == 'quest':
        print('ğŸ’¡ğŸ’¡ğŸ’¡ Quest')
        # åŠ å…¥ quest çš„è·¯å¾„
        current_dir = os.path.dirname(os.path.abspath(__file__))
        quest_base_dir = os.path.abspath(os.path.join(current_dir, "../../baselines/quest"))
        if quest_base_dir not in sys.path:
            sys.path.insert(0, quest_base_dir)

        from baselines.quest.evaluation.llama import enable_tuple_kv_cache_for_llama
        from baselines.quest.evaluation.quest_attention import enable_quest_attention_eval

        # å¯ç”¨ Quest æ‰€éœ€çš„è¡¥ä¸ï¼Œæ”¹å˜ KV Cache å¤„ç†æ–¹å¼
        enable_tuple_kv_cache_for_llama()

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=cuda_device,
            trust_remote_code=True,
            attn_implementation="flash_attention_2",
        )

        class QuestArgs:
            def __init__(self, token_budget, chunk_size):
                self.token_budget = token_budget
                self.chunk_size = chunk_size

        # ä» infer_config ä¸­è·å–å‚æ•°
        quest_args = QuestArgs(
            token_budget=infer_config['num_top_tokens'],
            chunk_size=infer_config.get('chunk_size', 16)
        )
        enable_quest_attention_eval(model, quest_args)

    elif model_cls == 'palu':
        print('ğŸ’¡ğŸ’¡ğŸ’¡ Palu')
        import transformers
        assert transformers.__version__ == '4.37.2'

        current_dir = os.path.dirname(os.path.abspath(__file__))
        palu_base_dir = os.path.abspath(os.path.join(current_dir, "../../baselines/palu"))
        if palu_base_dir not in sys.path:
            sys.path.insert(0, palu_base_dir)

        # å¿…é¡»å…ˆå¯¼å…¥ palu.model ä»¥è§¦å‘ AutoConfig/AutoModel æ³¨å†Œ
        import palu.model  # noqa: F401
        from palu.quant_utils import configure_latent_quantizer

        # Palu æ¨¡å‹é€šå¸¸ä½¿ç”¨ float16ï¼Œä¸”ä¾èµ–å…¶è‡ªå®šä¹‰çš„ Triton kernel
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map=cuda_device,
            trust_remote_code=True,
            attn_implementation="flash_attention_2",
        )

        # å¦‚æœé…ç½®äº†ä½æ¯”ç‰¹é‡åŒ–ï¼ˆlt_bits < 16ï¼‰ï¼Œåˆ™è¿›è¡Œé…ç½®
        lt_bits = infer_config.get('lt_bits', 16)
        if lt_bits < 16:
            configure_latent_quantizer(
                model,
                n_bits=lt_bits,
                group_size=infer_config.get('lt_group_size', 0),
                sym=infer_config.get('lt_sym', True),
                clip_ratio=infer_config.get('lt_clip_ratio', 1.0),
                hadamard=infer_config.get('lt_hadamard', False)
            )

    elif model_cls == 'kivi':
        print('ğŸ’¡ğŸ’¡ğŸ’¡ KIVI')
        current_dir = os.path.dirname(os.path.abspath(__file__))
        kivi_base_dir = os.path.abspath(os.path.join(current_dir, "../../baselines/kivi"))
        if kivi_base_dir not in sys.path:
            sys.path.insert(0, kivi_base_dir)

        base_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        
        # é…ç½® KIVI å‚æ•°
        base_config.k_bits = infer_config.get('k_bits', 2)
        base_config.v_bits = infer_config.get('v_bits', 2)
        base_config.group_size = infer_config.get('group_size', 32)
        base_config.residual_length = infer_config.get('residual_length', 32)
        base_config.use_flash = True

        if base_config.model_type == 'llama' or 'llama' in model_path.lower():
            from models.llama_kivi import LlamaForCausalLM_KIVI as KVModel
        elif base_config.model_type == 'mistral' or 'mistral' in model_path.lower():
            from models.mistral_kivi import MistralForCausalLM_KIVI as KVModel
        else:
            raise ValueError(f"KIVI does not support model type: {base_config.model_type}")

        model = KVModel.from_pretrained(
            model_path,
            config=base_config,
            torch_dtype=torch.float16,
            device_map=cuda_device,
            low_cpu_mem_usage=True,
        )
    elif model_cls == 'adakv':
        print('ğŸ’¡ğŸ’¡ğŸ’¡ AdaKV')
        os.environ['ENABLE_HF_GEN'] = '1'  # ç”Ÿæˆhackæµç¨‹ä¾èµ–äºgenerateå‡½æ•°
        current_dir = os.path.dirname(os.path.abspath(__file__))
        adakv_base_dir = os.path.abspath(os.path.join(current_dir, "../../baselines/adakv"))
        if adakv_base_dir not in sys.path:
            sys.path.insert(0, adakv_base_dir)

        from adaptive_snapkv.monkeypatch.monkeypatch import (
            replace_mistral_adaptive, replace_llama_adaptive,
            replace_mistral_fixed, replace_llama_fixed,
            config_compress
        )

        base_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        use_adaptive = infer_config.get('use_adaptive', True)

        if base_config.model_type == 'llama':
            if use_adaptive:
                replace_llama_adaptive()
            else:
                replace_llama_fixed()
        elif base_config.model_type == 'mistral':
            if use_adaptive:
                replace_mistral_adaptive()
            else:
                replace_mistral_fixed()
        else:
            raise ValueError(f"AdaKV does not support model type: {base_config.model_type}")

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=cuda_device,
            attn_implementation="flash_attention_2",
            trust_remote_code=True,
        )

        # config hyperparameters
        model = config_compress(
            model,
            window_size=infer_config.get('snapkv_window_size', 32),
            base_capacity=infer_config.get('num_top_tokens', 512),
            kernel_size=infer_config.get('kernel_size', 7),
            pooling=infer_config.get('pooling', 'maxpool'),
            floor_alpha=infer_config.get('floor_alpha', 0.2),
            pyram_mode=infer_config.get('pyram_mode', False),
            beta=infer_config.get('pyram_beta', 20),
            gqa_support=infer_config.get('gqa_support', False),
            gqa_func=infer_config.get('gqa_func', 'mean')
        )
    else:
        raise ValueError(f"Unknown model_cls: {model_cls}")

    chunk_prefill_size = int(os.environ.get('MANUAL_GEN_CHUNK_PREFILL_SIZE', 0))
    if chunk_prefill_size > 0:
        assert model_cls == 'kivi' or model_cls == 'palu' or model_cls == 'quest', 'å…¶ä»–æ–¹æ³•åœ¨ä»£ç å†…éƒ¨å®ç°äº†chunk prefill'

    model.eval()
    if tokenizer_path is None:
        tokenizer_path = model_path
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left'

    def generate(prompt: Union[str, List[str]], past_key_values=None, **kwargs):
        if os.getenv('ENABLE_HF_GEN'):
            return hf_gen(model, tokenizer, prompt, return_kv_cache, past_key_values, **kwargs)
        else:
            return manual_generate(model, tokenizer, prompt, past_key_values, return_kv_cache, **kwargs)

    if return_model:
        return generate, model

    return generate